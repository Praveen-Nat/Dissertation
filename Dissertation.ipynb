{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXTRACTION AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "input_folder = '/home/praveen-murugan/Downloads/TM_B13/data/'\n",
    "output_folder = '/home/praveen-murugan/Downloads/TM_B13/data/jpg'\n",
    "\n",
    "\n",
    "substrings = ['_True_color', 'B06', 'B07', 'B08', 'B09']\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if any(substring in file_name for substring in substrings):\n",
    "        tiff_path = os.path.join(input_folder, file_name)\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(tiff_path) as src:\n",
    "               \n",
    "                band = src.read(1)\n",
    "                \n",
    "                \n",
    "                if band.max() > 255:\n",
    "                    band = np.interp(band, (band.min(), band.max()), (0, 255))\n",
    "                band = band.astype(np.uint8)\n",
    "                \n",
    "               \n",
    "                img = Image.fromarray(band)\n",
    "                jpg_file_name = os.path.splitext(file_name)[0] + '.jpg'\n",
    "                jpg_path = os.path.join(output_folder, jpg_file_name)\n",
    "                \n",
    "                img.save(jpg_path, 'JPEG')\n",
    "                \n",
    "                print(f\"Converted {file_name} to {jpg_file_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_name}: {e}\")\n",
    "\n",
    "print(\"Conversion completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAND MASK APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "\n",
    "\n",
    "input_dir = '/home/praveen-murugan/Downloads/TM_B13/zips/'  \n",
    "output_dir = '/home/praveen-murugan/Downloads/TM_B13/data/'  \n",
    "mask_path = '/home/praveen-murugan/Downloads/TM_B13/mask/nland_mask.png'  \n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#  mask \n",
    "def apply_mask(image, mask):\n",
    "  \n",
    "    if mask.ndim == 3:\n",
    "        mask = mask[:, :, 0]\n",
    "    \n",
    "   \n",
    "    if len(image.shape) == 3:\n",
    "        mask = np.repeat(mask[np.newaxis, :, :], image.shape[0], axis=0)\n",
    "    \n",
    "    masked_image = np.where(mask == 0, image, 0)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def process_zip_file(zip_file_path, mask):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "       \n",
    "        temp_dir = os.path.join(output_dir, os.path.basename(zip_file_path).replace('.zip', ''))\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        zip_ref.extractall(temp_dir)\n",
    "       \n",
    "        image_paths = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.tiff')]\n",
    "        \n",
    "        for image_path in image_paths:\n",
    "        \n",
    "            with rasterio.open(image_path) as src:\n",
    "                image = src.read()\n",
    "                profile = src.profile\n",
    "\n",
    "             \n",
    "                masked_image = apply_mask(image, mask)\n",
    "\n",
    "                output_path = os.path.join(output_dir, f'{os.path.basename(zip_file_path).replace(\".zip\", \"\")}_{os.path.basename(image_path)}')\n",
    "                with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                    dst.write(masked_image)\n",
    "\n",
    "mask = np.array(Image.open(mask_path))\n",
    "\n",
    "for zip_file in os.listdir(input_dir):\n",
    "    if zip_file.endswith('.zip'):\n",
    "        process_zip_file(os.path.join(input_dir, zip_file), mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIR mask test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import rasterio\n",
    "\n",
    "\n",
    "data_dir = \"/home/praveen-murugan/Downloads/TM_B13/data\"\n",
    "\n",
    "def load_and_normalize_band(file_path):\n",
    "    \"\"\"Load and normalize a single band.\"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        band = src.read(1).astype(np.float32)\n",
    "       \n",
    "        band_normalized = (band - band.min()) / (band.max() - band.min())\n",
    "    return band_normalized\n",
    "\n",
    "def load_image_bands_and_normalize(folder_path):\n",
    "    \"\"\"Load and normalize RGB and NIR bands using rasterio.\"\"\"\n",
    "    band_files = {\n",
    "        'B02': None,\n",
    "        'B03': None,\n",
    "        'B04': None,\n",
    "        'B08': None  # NIR band\n",
    "    }\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if 'Sentinel-2_L2A_B02_(Raw)' in file_name:\n",
    "            band_files['B02'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B03_(Raw)' in file_name:\n",
    "            band_files['B03'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B04_(Raw)' in file_name:\n",
    "            band_files['B04'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B08_(Raw)' in file_name:\n",
    "            band_files['B08'] = os.path.join(folder_path, file_name)\n",
    "\n",
    "    missing_bands = [band for band, file_path in band_files.items() if file_path is None]\n",
    "    if missing_bands:\n",
    "        raise FileNotFoundError(f\"Missing bands: {', '.join(missing_bands)} in folder {folder_path}\")\n",
    "\n",
    "    normalized_bands = {band: load_and_normalize_band(file_path) for band, file_path in band_files.items()}\n",
    "\n",
    "    normalized_image = np.stack([normalized_bands['B02'], normalized_bands['B03'], normalized_bands['B04'], normalized_bands['B08']], axis=-1)\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "def preprocess_and_cluster_by_band(data_dir):\n",
    "    \"\"\"Load data, normalize bands, perform clustering, and generate masks.\"\"\"\n",
    "    image_data = []\n",
    "    mask_data = []\n",
    "\n",
    "    for folder_name in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "          \n",
    "            normalized_image = load_image_bands_and_normalize(folder_path)\n",
    "\n",
    "            \n",
    "            normalized_image = cv2.resize(normalized_image, (256, 256))\n",
    "\n",
    "            \n",
    "            nir_band = normalized_image[:, :, 3].reshape(-1, 1) \n",
    "\n",
    "         \n",
    "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "            kmeans.fit(nir_band)\n",
    "            clustered_image = kmeans.labels_.reshape((256, 256))\n",
    "\n",
    "           \n",
    "            unique, counts = np.unique(clustered_image, return_counts=True)\n",
    "            cluster_sizes = dict(zip(unique, counts))\n",
    "            sorted_clusters = sorted(cluster_sizes, key=cluster_sizes.get)\n",
    "\n",
    "          \n",
    "            hyacinth_cluster = sorted_clusters[0]\n",
    "\n",
    "            mask = (clustered_image == hyacinth_cluster).astype(np.uint8)\n",
    "\n",
    "            image_data.append(normalized_image)\n",
    "            mask_data.append(mask)\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"Error processing folder {folder_name}: {e}\")\n",
    "\n",
    "    return np.array(image_data), np.array(mask_data)\n",
    "\n",
    "X_data, y_data = preprocess_and_cluster_by_band(data_dir)\n",
    "\n",
    "\n",
    "y_data = np.expand_dims(y_data, axis=-1)\n",
    "\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "num_files = 38  \n",
    "\n",
    "for i in range(num_files):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Original Image {i+1}\")\n",
    "    plt.imshow(X_data[i][:, :, :3])  \n",
    "    plt.axis('off')\n",
    "\n",
    "   \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"NIR Band {i+1}\")\n",
    "    plt.imshow(X_data[i][:, :, 3], cmap='gray')  \n",
    "    plt.axis('off')\n",
    "\n",
    " \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"K-Means Mask {i+1} (NIR Band Only)\")\n",
    "    plt.imshow(y_data[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # UNET setup with RGB Bands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score, confusion_matrix\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "import albumentations as A\n",
    "import seaborn as sns\n",
    "\n",
    "# Set data directories\n",
    "data_dir = \"/home/praveen-murugan/Downloads/TM_B13/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for Loading and Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_band(file_path):\n",
    "    \"\"\"Load and normalize a single band.\"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        band = src.read(1).astype(np.float32)\n",
    "      \n",
    "        band_normalized = (band - band.min()) / (band.max() - band.min())\n",
    "    return band_normalized\n",
    "\n",
    "def load_image_bands_and_normalize(folder_path):\n",
    "    \"\"\"Load and normalize RGB bands using rasterio.\"\"\"\n",
    "    band_files = {\n",
    "        'B02': None,\n",
    "        'B03': None,\n",
    "        'B04': None\n",
    "    }\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if 'Sentinel-2_L2A_B02_(Raw)' in file_name:\n",
    "            band_files['B02'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B03_(Raw)' in file_name:\n",
    "            band_files['B03'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B04_(Raw)' in file_name:\n",
    "            band_files['B04'] = os.path.join(folder_path, file_name)\n",
    "\n",
    "  \n",
    "    missing_bands = [band for band, file_path in band_files.items() if file_path is None]\n",
    "    if missing_bands:\n",
    "        raise FileNotFoundError(f\"Missing bands: {', '.join(missing_bands)} in folder {folder_path}\")\n",
    "\n",
    "    normalized_bands = {band: load_and_normalize_band(file_path) for band, file_path in band_files.items()}\n",
    "\n",
    "    normalized_image = np.stack([normalized_bands['B02'], normalized_bands['B03'], normalized_bands['B04']], axis=-1)\n",
    "\n",
    "    return normalized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_cluster_by_band(data_dir):\n",
    "    \"\"\"Load data, normalize bands, perform clustering, and apply augmentation.\"\"\"\n",
    "    image_data = []\n",
    "    mask_data = []\n",
    "\n",
    "    for folder_name in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "         \n",
    "            normalized_image = load_image_bands_and_normalize(folder_path)\n",
    "\n",
    "   \n",
    "            normalized_image = cv2.resize(normalized_image, (256, 256))\n",
    "\n",
    "            image_flat = normalized_image.reshape(-1, 3)\n",
    "\n",
    "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "            kmeans.fit(image_flat)\n",
    "            clustered_image = kmeans.labels_.reshape((256, 256))\n",
    "\n",
    "       \n",
    "            unique, counts = np.unique(clustered_image, return_counts=True)\n",
    "            cluster_sizes = dict(zip(unique, counts))\n",
    "            sorted_clusters = sorted(cluster_sizes, key=cluster_sizes.get)\n",
    "\n",
    "         \n",
    "            hyacinth_cluster = sorted_clusters[0]\n",
    "\n",
    "            mask = (clustered_image == hyacinth_cluster).astype(np.uint8)\n",
    "\n",
    "           \n",
    "            image_data.append(normalized_image)\n",
    "            mask_data.append(mask)\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"Error processing folder {folder_name}: {e}\")\n",
    "\n",
    "    return np.array(image_data), np.array(mask_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_data, mask_data = preprocess_and_cluster_by_band(data_dir)\n",
    "\n",
    "mask_data = np.expand_dims(mask_data, axis=-1)\n",
    "\n",
    "mask_data = mask_data.astype(np.float32)\n",
    "\n",
    "\n",
    "print(f\"Total number of images: {len(image_data)}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.imshow(image_data[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.imshow(mask_data[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Custom Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_data, mask_data, batch_size, augmentations):\n",
    "        self.image_data = image_data\n",
    "        self.mask_data = mask_data\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentations = augmentations\n",
    "        self.indices = np.arange(len(image_data))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_data) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = self.image_data[batch_indices]\n",
    "        batch_masks = self.mask_data[batch_indices]\n",
    "        \n",
    "        augmented_images = []\n",
    "        augmented_masks = []\n",
    "        \n",
    "        for img, mask in zip(batch_images, batch_masks):\n",
    "            augmented = self.augmentations(image=img, mask=mask)\n",
    "            augmented_images.append(augmented['image'])\n",
    "            augmented_masks.append(augmented['mask'])\n",
    "        \n",
    "        return np.array(augmented_images), np.array(augmented_masks)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def unet(pretrained_weights=None, input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[dice_coef])\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "iou_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    true_mask = y_val[i].squeeze().flatten()\n",
    "    pred_mask = predicted_masks[i].squeeze().flatten()\n",
    "\n",
    "    precision = precision_score(true_mask, pred_mask)\n",
    "    recall = recall_score(true_mask, pred_mask)\n",
    "    f1 = f1_score(true_mask, pred_mask)\n",
    "    iou = jaccard_score(true_mask, pred_mask)\n",
    "    cm = confusion_matrix(true_mask, pred_mask)\n",
    "\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    iou_scores.append(iou)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "precision_scores = np.array(precision_scores)\n",
    "recall_scores = np.array(recall_scores)\n",
    "f1_scores = np.array(f1_scores)\n",
    "iou_scores = np.array(iou_scores)\n",
    "\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Average IoU: {np.mean(iou_scores):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Sample Confusion Matrix:\")\n",
    "print(confusion_matrices[0])\n",
    "\n",
    "\n",
    "def overlay_masks(image, mask):\n",
    "    overlay = image.copy()\n",
    "    overlay[mask[:, :, 0] == 1] = [255, 0, 0]  \n",
    "    return overlay\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(X_val[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted Mask Overlay\")\n",
    "    overlay = overlay_masks(X_val[i], predicted_masks[i])\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrices[0], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for First Sample\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = unet(input_size=(256, 256, 3))\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_data, mask_data, test_size=0.2, random_state=42)\n",
    "\n",
    "augmentations = A.Compose([\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0.1, p=0.5)\n",
    "])\n",
    "\n",
    "\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=4, augmentations=augmentations)\n",
    "val_generator = DataGenerator(X_val, y_val, batch_size=4, augmentations=augmentations)\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('1unet_water_hyacinth.keras', monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=50,\n",
    "          validation_data=val_generator,\n",
    "          callbacks=[model_checkpoint])\n",
    "\n",
    "model = unet(input_size=(256, 256, 3))\n",
    "model.load_weights('1unet_water_hyacinth.keras')\n",
    "\n",
    "\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_masks = (predictions > threshold).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET setup with RGB & NIR Bands "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Band Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score, confusion_matrix\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "import albumentations as A\n",
    "import seaborn as sns\n",
    "\n",
    "data_directory = \"/home/praveen-murugan/Downloads/TM_B13/data\"\n",
    "\n",
    "def load_normalize_band(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        band_data = src.read(1).astype(np.float32)\n",
    "        normalized_band = (band_data - band_data.min()) / (band_data.max() - band_data.min())\n",
    "    return normalized_band\n",
    "\n",
    "def load_normalize_bands(folder_path):\n",
    "    band_files = {\n",
    "        'blue': None,\n",
    "        'green': None,\n",
    "        'red': None,\n",
    "        'nir': None\n",
    "    }\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if 'Sentinel-2_L2A_B02_(Raw)' in file_name:\n",
    "            band_files['blue'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B03_(Raw)' in file_name:\n",
    "            band_files['green'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B04_(Raw)' in file_name:\n",
    "            band_files['red'] = os.path.join(folder_path, file_name)\n",
    "        elif 'Sentinel-2_L2A_B08_(Raw)' in file_name:\n",
    "            band_files['nir'] = os.path.join(folder_path, file_name)\n",
    "\n",
    "    missing_bands = [band for band, file_path in band_files.items() if file_path is None]\n",
    "    if missing_bands:\n",
    "        raise FileNotFoundError(f\"Missing bands: {', '.join(missing_bands)} in folder {folder_path}\")\n",
    "\n",
    "    normalized_bands = {band: load_normalize_band(file_path) for band, file_path in band_files.items()}\n",
    "    normalized_image = np.stack([normalized_bands['blue'], normalized_bands['green'], normalized_bands['red'], normalized_bands['nir']], axis=-1)\n",
    "    return normalized_image\n",
    "\n",
    "def process_and_cluster(data_directory):\n",
    "    image_list = []\n",
    "    mask_list = []\n",
    "\n",
    "    for folder_name in os.listdir(data_directory):\n",
    "        folder_path = os.path.join(data_directory, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            normalized_image = load_normalize_bands(folder_path)\n",
    "            resized_image = cv2.resize(normalized_image, (256, 256))\n",
    "            nir_band = resized_image[:, :, 3].reshape(-1, 1)\n",
    "\n",
    "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "            kmeans.fit(nir_band)\n",
    "            clustered_image = kmeans.labels_.reshape((256, 256))\n",
    "\n",
    "            unique, counts = np.unique(clustered_image, return_counts=True)\n",
    "            cluster_sizes = dict(zip(unique, counts))\n",
    "            sorted_clusters = sorted(cluster_sizes, key=cluster_sizes.get)\n",
    "\n",
    "            hyacinth_cluster = sorted_clusters[0]\n",
    "            hyacinth_mask = (clustered_image == hyacinth_cluster).astype(np.uint8)\n",
    "\n",
    "            image_list.append(resized_image)\n",
    "            mask_list.append(hyacinth_mask)\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            print(f\"Error processing folder {folder_name}: {e}\")\n",
    "\n",
    "    return np.array(image_list), np.array(mask_list)\n",
    "\n",
    "image_data, mask_data = process_and_cluster(data_directory)\n",
    "\n",
    "mask_data = np.expand_dims(mask_data, axis=-1)\n",
    "mask_data = mask_data.astype(np.float32)\n",
    "\n",
    "print(f\"Total number of images: {len(image_data)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.imshow(image_data[0][:, :, :3])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.imshow(mask_data[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, images, masks, batch_size, augmentations):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentations = augmentations\n",
    "        self.indices = np.arange(len(images))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = self.images[batch_indices]\n",
    "        batch_masks = self.masks[batch_indices]\n",
    "        \n",
    "        augmented_images = []\n",
    "        augmented_masks = []\n",
    "        \n",
    "        for img, mask in zip(batch_images, batch_masks):\n",
    "            augmented = self.augmentations(image=img, mask=mask)\n",
    "            augmented_images.append(augmented['image'])\n",
    "            augmented_masks.append(augmented['mask'])\n",
    "        \n",
    "        return np.array(augmented_images), np.array(augmented_masks)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_flat = K.flatten(y_true)\n",
    "    y_pred_flat = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_flat * y_pred_flat)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_flat) + K.sum(y_pred_flat) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def unet_model(pretrained_weights=None, input_size=(256, 256, 4)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    output = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss=dice_loss, metrics=[dice_coefficient])\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model(input_size=(256, 256, 4))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_data, mask_data, test_size=0.2, random_state=42)\n",
    "\n",
    "augmentations = A.Compose([\n",
    "    A.Rotate(limit=90, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0.1, p=0.5)\n",
    "])\n",
    "\n",
    "train_generator = CustomDataGenerator(X_train, y_train, batch_size=4, augmentations=augmentations)\n",
    "val_generator = CustomDataGenerator(X_val, y_val, batch_size=4, augmentations=augmentations)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('unet_water_hyacinth.keras', monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=50,\n",
    "          validation_data=val_generator,\n",
    "          callbacks=[model_checkpoint])\n",
    "\n",
    "model = unet_model(input_size=(256, 256, 4))\n",
    "model.load_weights('unet_water_hyacinth.keras')\n",
    "\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_masks = (predictions > threshold).astype(np.uint8)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "iou_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    true_mask = y_val[i].squeeze().flatten()\n",
    "    pred_mask = predicted_masks[i].squeeze().flatten()\n",
    "\n",
    "    precision = precision_score(true_mask, pred_mask)\n",
    "    recall = recall_score(true_mask, pred_mask)\n",
    "    f1 = f1_score(true_mask, pred_mask)\n",
    "    iou = jaccard_score(true_mask, pred_mask)\n",
    "    cm = confusion_matrix(true_mask, pred_mask)\n",
    "\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    iou_scores.append(iou)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "precision_scores = np.array(precision_scores)\n",
    "recall_scores = np.array(recall_scores)\n",
    "f1_scores = np.array(f1_scores)\n",
    "iou_scores = np.array(iou_scores)\n",
    "\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Average IoU: {np.mean(iou_scores):.4f}\")\n",
    "\n",
    "print(\"Sample Confusion Matrix:\")\n",
    "print(confusion_matrices[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_predicted_mask(image, mask):\n",
    "    overlay = image[:, :, :3].copy()\n",
    "    overlay[mask[:, :, 0] == 1] = [255, 0, 0]\n",
    "    return overlay\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(X_val[i][:, :, :3])\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.imshow(y_val[i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted Mask Overlay\")\n",
    "    overlay = overlay_predicted_mask(X_val[i], predicted_masks[i])\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrices[0], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for First Sample\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
